{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"anna.txt\", 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 22, 72, 15, 57, 74, 81, 10, 76, 44, 44, 44,  3, 72, 15, 15, 64,\n",
       "       10, 53, 72, 34, 60, 33, 60, 74, 26, 10, 72, 81, 74, 10, 72, 33, 33,\n",
       "       10, 72, 33, 60, 27, 74, 56, 10, 74, 59, 74, 81, 64, 10, 14, 63, 22,\n",
       "       72, 15, 15, 64, 10, 53, 72, 34, 60, 33, 64, 10, 60, 26, 10, 14, 63,\n",
       "       22, 72, 15, 15, 64, 10, 60, 63, 10, 60, 57, 26, 10, 67, 78, 63, 44,\n",
       "       78, 72, 64, 31, 44, 44, 21, 59, 74, 81, 64, 57, 22, 60, 63])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch : ii for ii, ch in int2char.items()}\n",
    "\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'F', 1: '/', 2: 'W', 3: 'H', 4: 'O', 5: 'Q', 6: '!', 7: 'M', 8: ':', 9: 'L', 10: ' ', 11: 'z', 12: 'K', 13: '%', 14: 'u', 15: 'p', 16: '*', 17: 'C', 18: 'b', 19: '$', 20: 'G', 21: 'E', 22: 'h', 23: 'N', 24: '_', 25: 'U', 26: 's', 27: 'k', 28: 'I', 29: '-', 30: 'Z', 31: '.', 32: '6', 33: 'l', 34: 'm', 35: 'd', 36: 'j', 37: '0', 38: '8', 39: ')', 40: 'c', 41: 'D', 42: 'x', 43: '2', 44: '\\n', 45: 'Y', 46: 'A', 47: 'S', 48: 'R', 49: 'V', 50: '7', 51: '?', 52: '`', 53: 'f', 54: 'J', 55: 'q', 56: ';', 57: 't', 58: '3', 59: 'v', 60: 'i', 61: '4', 62: '&', 63: 'n', 64: 'y', 65: '9', 66: 'X', 67: 'o', 68: 'g', 69: 'T', 70: ',', 71: '(', 72: 'a', 73: 'B', 74: 'e', 75: '5', 76: '1', 77: 'P', 78: 'w', 79: \"'\", 80: '\"', 81: 'r', 82: '@'}\n"
     ]
    }
   ],
   "source": [
    "print (int2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((arr.shape[0]*arr.shape[1], n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    one_hot = one_hot.reshape((arr.shape[0],arr.shape[1], n_labels))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1.]\n",
      "  [0. 1. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = np.array([[3, 5, 1]])\n",
    "print (one_hot_encode(test_seq, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_sz, seq_length):\n",
    "    batch_sz_total = batch_sz * seq_length\n",
    "    n_batches = len(arr) // batch_sz_total\n",
    "    arr = arr[:n_batches*batch_sz_total]\n",
    "    arr = arr.reshape((batch_sz,-1))\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[: ,n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[17 22 72 15 57 74 81 10 76 44]\n",
      " [26 67 63 10 57 22 72 57 10 72]\n",
      " [74 63 35 10 67 81 10 72 10 53]\n",
      " [26 10 57 22 74 10 40 22 60 74]\n",
      " [10 26 72 78 10 22 74 81 10 57]\n",
      " [40 14 26 26 60 67 63 10 72 63]\n",
      " [10 46 63 63 72 10 22 72 35 10]\n",
      " [ 4 18 33 67 63 26 27 64 31 10]]\n",
      "\n",
      "y\n",
      " [[22 72 15 57 74 81 10 76 44 44]\n",
      " [67 63 10 57 22 72 57 10 72 57]\n",
      " [63 35 10 67 81 10 72 10 53 67]\n",
      " [10 57 22 74 10 40 22 60 74 53]\n",
      " [26 72 78 10 22 74 81 10 57 74]\n",
      " [14 26 26 60 67 63 10 72 63 35]\n",
      " [46 63 63 72 10 22 72 35 10 26]\n",
      " [18 33 67 63 26 27 64 31 10 80]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)\n",
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import LSTM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self,tokens,n_hiddens=256,n_layers=2,drop_prob=0.5,lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob=drop_prob\n",
    "        self.n_layers=n_layers\n",
    "        self.n_hidden=n_hidden\n",
    "        self.lr=lr\n",
    "        self.chars=tokens\n",
    "        self.int2char= dict(enumerate(self.chars))\n",
    "        self.char2int={ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = LSTM(input_size = len(self.chars),\n",
    "                         hidden_size = n_hiddens ,\n",
    "                         num_layers = n_layers, \n",
    "                         batch_first = True,\n",
    "                         dropout = drop_prob)\n",
    "        self.fc = nn.Linear(n_hiddens, len(self.chars))\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out , hidden = self.lstm(x,hidden)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "#         weight = next(self.parameters()).data\n",
    "#         h = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "#              weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        h = (torch.zeros(self.n_layers, batch_size, self.n_hidden, device=device),\n",
    "            torch.zeros(self.n_layers, batch_size, self.n_hidden, device=device))\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "a=torch.zeros(1,1,1, 10)\n",
    "print (a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs= 10, batch_size = 10, \n",
    "          seq_length = 50, lr = 0.001, clip = 5, \n",
    "          val_frac = 0.1, print_every = 10):\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr= lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    net.to(device)\n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            output, h = net(inputs, h)\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 2.0822... Val Loss: 2.0070\n",
      "Epoch: 2/20... Step: 20... Loss: 1.7177... Val Loss: 1.6776\n",
      "Epoch: 2/20... Step: 30... Loss: 1.5399... Val Loss: 1.5364\n",
      "Epoch: 3/20... Step: 40... Loss: 1.4430... Val Loss: 1.4724\n",
      "Epoch: 3/20... Step: 50... Loss: 1.3792... Val Loss: 1.4318\n",
      "Epoch: 4/20... Step: 60... Loss: 1.3466... Val Loss: 1.4009\n",
      "Epoch: 5/20... Step: 70... Loss: 1.3138... Val Loss: 1.3761\n",
      "Epoch: 5/20... Step: 80... Loss: 1.2916... Val Loss: 1.3612\n",
      "Epoch: 6/20... Step: 90... Loss: 1.2659... Val Loss: 1.3513\n",
      "Epoch: 6/20... Step: 100... Loss: 1.2435... Val Loss: 1.3593\n",
      "Epoch: 7/20... Step: 110... Loss: 1.2426... Val Loss: 1.3407\n",
      "Epoch: 8/20... Step: 120... Loss: 1.2755... Val Loss: 1.3267\n",
      "Epoch: 8/20... Step: 130... Loss: 1.2117... Val Loss: 1.3162\n",
      "Epoch: 9/20... Step: 140... Loss: 1.2019... Val Loss: 1.3105\n",
      "Epoch: 9/20... Step: 150... Loss: 1.1980... Val Loss: 1.3085\n",
      "Epoch: 10/20... Step: 160... Loss: 1.1909... Val Loss: 1.3096\n",
      "Epoch: 10/20... Step: 170... Loss: 1.2244... Val Loss: 1.3054\n",
      "Epoch: 11/20... Step: 180... Loss: 1.1668... Val Loss: 1.3029\n",
      "Epoch: 12/20... Step: 190... Loss: 1.1573... Val Loss: 1.2994\n",
      "Epoch: 12/20... Step: 200... Loss: 1.1539... Val Loss: 1.2947\n",
      "Epoch: 13/20... Step: 210... Loss: 1.1425... Val Loss: 1.2899\n",
      "Epoch: 13/20... Step: 220... Loss: 1.1320... Val Loss: 1.2868\n",
      "Epoch: 14/20... Step: 230... Loss: 1.1304... Val Loss: 1.2870\n",
      "Epoch: 15/20... Step: 240... Loss: 1.1236... Val Loss: 1.2800\n",
      "Epoch: 15/20... Step: 250... Loss: 1.1266... Val Loss: 1.2836\n",
      "Epoch: 16/20... Step: 260... Loss: 1.1175... Val Loss: 1.2813\n",
      "Epoch: 16/20... Step: 270... Loss: 1.1072... Val Loss: 1.2792\n",
      "Epoch: 17/20... Step: 280... Loss: 1.1011... Val Loss: 1.2800\n",
      "Epoch: 18/20... Step: 290... Loss: 1.1457... Val Loss: 1.2761\n",
      "Epoch: 18/20... Step: 300... Loss: 1.1035... Val Loss: 1.2781\n",
      "Epoch: 19/20... Step: 310... Loss: 1.1014... Val Loss: 1.2752\n",
      "Epoch: 19/20... Step: 320... Loss: 1.0992... Val Loss: 1.2735\n",
      "Epoch: 20/20... Step: 330... Loss: 1.0969... Val Loss: 1.2746\n",
      "Epoch: 20/20... Step: 340... Loss: 1.1420... Val Loss: 1.2778\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.02, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda <class 'torch.device'>\n"
     ]
    }
   ],
   "source": [
    "# 7 : 1.62\n",
    "print (device, type(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(device == torch.device(\"cuda\")):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    net.to(device)\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna, as they\n",
      "will appreciate in the first minute. And have you been\n",
      "unable to come.\"\n",
      "\n",
      "\"Oh, yes, time in that is or taking off his soul.\"\n",
      "\n",
      "Alexey Alexandrovitch was standing at the carden finger. The more they would say to their mouth, and to arrange the standal and what\n",
      "he was not, happy from the study of his she forgotten, was there and shakon and hurriedly\n",
      "into the balance of the\n",
      "more strangers. Besides to the peasant, the children were satisfied, and turning harvesting ages or without him about the\n",
      "forest over her strange, and at once. Alexey\n",
      "Alexandrovitch had not an intellegent frost to see him to her, and almost all that strange file all of her side of\n",
      "something, and had never seen her sister, the most chalk of the prince, the most passages of\n",
      "the meaning\n",
      "of the memory of the classion, and was, as though he had never had to say something.\n",
      "\n",
      "\"I'm afraid! All right, about it!\" she said, \"that's there are the club to her, but I am very well, to touch her father, anyway,\" he said, and she\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (AI)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
