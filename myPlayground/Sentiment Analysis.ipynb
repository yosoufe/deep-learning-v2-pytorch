{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelfile = \"data/sentimentLabels.txt\"\n",
    "reviewfile = \"data/sentimentReviews.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(labelfile,'r') as f:\n",
    "    labelsRaw = f.read()\n",
    "with open(reviewfile,'r') as f:\n",
    "    reviewsRaw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "reviewsWithouTPunct = ''.join (c for c in reviewsRaw if c not in punctuation or c =='.')\n",
    "print (reviewsWithouTPunct[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive', 'negative']\n",
      "[['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', '.', 'my', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'high', 's', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', '.', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', '.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'at', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'high', '.', 'a', 'classic', 'line', 'inspector', 'i', 'm', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', '.', 'student', 'welcome', 'to', 'bromwell', 'high', '.', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', '.', 'what', 'a', 'pity', 'that', 'it', 'isn', 't'], ['story', 'of', 'a', 'man', 'who', 'has', 'unnatural', 'feelings', 'for', 'a', 'pig', '.', 'starts', 'out', 'with', 'a', 'opening', 'scene', 'that', 'is', 'a', 'terrific', 'example', 'of', 'absurd', 'comedy', '.', 'a', 'formal', 'orchestra', 'audience', 'is', 'turned', 'into', 'an', 'insane', 'violent', 'mob', 'by', 'the', 'crazy', 'chantings', 'of', 'it', 's', 'singers', '.', 'unfortunately', 'it', 'stays', 'absurd', 'the', 'whole', 'time', 'with', 'no', 'general', 'narrative', 'eventually', 'making', 'it', 'just', 'too', 'off', 'putting', '.', 'even', 'those', 'from', 'the', 'era', 'should', 'be', 'turned', 'off', '.', 'the', 'cryptic', 'dialogue', 'would', 'make', 'shakespeare', 'seem', 'easy', 'to', 'a', 'third', 'grader', '.', 'on', 'a', 'technical', 'level', 'it', 's', 'better', 'than', 'you', 'might', 'think', 'with', 'some', 'good', 'cinematography', 'by', 'future', 'great', 'vilmos', 'zsigmond', '.', 'future', 'stars', 'sally', 'kirkland', 'and', 'frederic', 'forrest', 'can', 'be', 'seen', 'briefly', '.']]\n"
     ]
    }
   ],
   "source": [
    "labels = labelsRaw.splitlines()\n",
    "revs = reviewsWithouTPunct.splitlines()\n",
    "\n",
    "all_words= list()\n",
    "\n",
    "reviews = list()\n",
    "for review in revs:\n",
    "    reviews.append(review.split())\n",
    "    all_words += reviews[-1]\n",
    "\n",
    "\n",
    "print(labels[:2])\n",
    "print(reviews[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74073\n",
      "['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', '.', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', '.', 'my', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'high', 's', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', '.', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', '.', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i']\n"
     ]
    }
   ],
   "source": [
    "print (len(set(all_words)))\n",
    "print(all_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique words: 74073\n",
      "the most repeated words: [('the', 336713), ('.', 327192), ('and', 164107), ('a', 163009), ('of', 145864), ('to', 135720), ('is', 107328), ('br', 101872), ('it', 96352), ('in', 93968)]\n",
      "some least repeated words [('whelk', 1), ('hued', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(all_words)\n",
    "srt = c.most_common()\n",
    "print (\"number of unique words:\", len(srt))\n",
    "print (\"the most repeated words:\", srt[:10])\n",
    "print (\"some least repeated words\", srt[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab sample ['the', '.', 'and', 'a', 'of', 'to', 'is', 'br', 'it', 'in']\n"
     ]
    }
   ],
   "source": [
    "vocab = [word for word,_ in srt]\n",
    "print (\"vocab sample\" , vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int = {word: i+1 for i,word in enumerate(vocab)}\n",
    "int2word = {i+1: word for i,word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode reviews\n",
    "reviews_int = [[word2int[w] for w in review] for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21026\n",
      "[[21026, 309, 7, 4, 1051, 208, 2, 9, 2139, 33, 1, 172, 58, 16, 50, 82, 5786, 45, 383, 111, 141, 16, 5195, 2, 61, 155, 10, 1, 4976, 5853, 476, 72, 6, 261, 13, 21026, 309, 14, 1979, 7, 75, 2396, 6, 614, 74, 7, 5195, 2, 1, 24104, 6, 1984, 10167, 1, 5787, 1500, 37, 52, 67, 205, 146, 68, 1200, 5195, 19870, 1, 37443, 5, 1, 222, 884, 32, 2989, 72, 5, 1, 5788, 11, 687, 3, 68, 1500, 2, 55, 11, 217, 1, 384, 10, 63, 4, 1407, 3687, 784, 6, 3484, 181, 1, 383, 11, 1213, 13584, 2, 2, 2, 2, 2, 2, 2, 2, 2, 33, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 309, 2, 4, 350, 342, 2914, 11, 144, 128, 6, 7691, 31, 5, 130, 5195, 2, 1407, 2327, 6, 21026, 309, 2, 11, 529, 13, 110, 1449, 5, 61, 544, 103, 13, 21026, 309, 7, 228, 4147, 2, 49, 4, 2212, 13, 9, 216, 24], [64, 5, 4, 126, 37, 48, 7473, 1396, 17, 4, 4182, 2, 506, 46, 18, 4, 623, 135, 13, 7, 4, 1280, 458, 5, 1722, 208, 2, 4, 10625, 7374, 301, 7, 668, 84, 36, 2117, 1087, 2990, 35, 1, 899, 46418, 5, 9, 14, 5097, 2, 465, 9, 2657, 1722, 1, 222, 58, 18, 59, 795, 1298, 833, 229, 9, 44, 99, 124, 1470, 2, 60, 148, 39, 1, 964, 143, 30, 668, 124, 2, 1, 13585, 411, 62, 95, 1775, 307, 756, 6, 4, 820, 10397, 2, 23, 4, 1725, 636, 9, 14, 129, 74, 22, 234, 103, 18, 50, 51, 618, 35, 683, 86, 28786, 28787, 2, 683, 375, 3342, 11399, 3, 16372, 7947, 52, 30, 109, 3325, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(word2int[reviews[0][0]])\n",
    "print(reviews_int[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(reviews_int[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode labels\n",
    "enc_labels = [1.0 if la =='positive' else 0.0 for la in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0]\n",
      "['positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative']\n"
     ]
    }
   ],
   "source": [
    "print(enc_labels[:10])\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length reviews: 0\n",
      "Maximum review length: 2633\n",
      "total reviews: 25000\n",
      "24989 samples with minimum length of 20\n"
     ]
    }
   ],
   "source": [
    "# remove outliers\n",
    "review_lens = Counter([len(x) for x in reviews_int])\n",
    "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
    "print(\"Maximum review length: {}\".format(max(review_lens)))\n",
    "print(\"total reviews:\", len(reviews_int))\n",
    "\n",
    "min_len = 20\n",
    "idx_desired_date = [idx for idx, rev in enumerate(reviews_int) if len(rev) >= min_len]\n",
    "print(\"{} samples with minimum length of {}\".format(len(idx_desired_date), min_len))\n",
    "\n",
    "filtered_revs = [reviews_int[idx] for idx in idx_desired_date ]\n",
    "filtered_labels = [enc_labels[idx] for idx in idx_desired_date ]\n",
    "\n",
    "reviews_int = filtered_revs\n",
    "enc_labels = filtered_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_ints, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each review, I grab that review and \n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [22383    43 46419    16   707 17140  3390    48    78    36]\n",
      " [ 4506   506    16     4  3343   163  8313  1653     7  4820]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   55    11    15   117    61   799   553    72   365     6]\n",
      " [    0     0    12   216    24     1  1687  2070  1566   868]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   331   579    35     4   163   749  2732    10   326]\n",
      " [   10    12 10172  5306  1947   690   445    23   281   674]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    1   308 10400  2070  1566  6203  6529  3289 17947 10629]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   22   123  2070  1566   516  8182    89     7  1326  1183]\n",
      " [    1    21     7    77     2    41     7    59    82    96]\n",
      " [   55    11    85   330 26231 46428    64    11    15   615]\n",
      " [   12    21     7    31  1437 32318  3770     2   691 15101]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   41    27   110 17953  1423    10     1   328     5   126]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [   11   500     1   308 10400    56    75     2     9    14]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]\n",
      " [    0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Test your implementation!\n",
    "\n",
    "seq_length = 200\n",
    "\n",
    "features = pad_features(reviews_int, seq_length=seq_length)\n",
    "\n",
    "## test statements - do not change - ##\n",
    "assert len(features)==len(reviews_int), \"Your features should have as many rows as reviews.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "l = features[:30,:10]\n",
    "print(np.array(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(19991, 200) \n",
      "Validation set: \t(2499, 200) \n",
      "Test set: \t\t(2499, 200)\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "split_frac = 0.8\n",
    "encoded_labels = np.array(enc_labels,dtype=np.float64)\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (19991, 200) <class 'numpy.ndarray'> (19991,)\n",
      "<class 'numpy.ndarray'> (2499, 200) <class 'numpy.ndarray'> (2499,)\n",
      "<class 'numpy.ndarray'> (2499, 200) <class 'numpy.ndarray'> (2499,)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_x), train_x.shape, type(train_y), train_y.shape)\n",
    "print(type(val_x), val_x.shape, type(val_y), val_y.shape)\n",
    "print(type(test_x), test_x.shape, type(test_y), test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "batch_size = 1024\n",
    "tr = int(train_x.shape[0]/batch_size)*batch_size\n",
    "va = int(val_x.shape[0]/batch_size)*batch_size\n",
    "te = int(test_x.shape[0]/batch_size)*batch_size\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_x[:tr]),torch.from_numpy(train_y[:tr]))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x[:va]),torch.from_numpy(val_y[:va]))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x[:te]),torch.from_numpy(test_y[:te]))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([1024, 200])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ...,    24,    67,     9],\n",
      "        [    0,     0,     0,  ...,     4,   153,     2],\n",
      "        [    0,     0,     0,  ...,    18,    12,     2],\n",
      "        ...,\n",
      "        [   11,    40,    12,  ...,     3, 18211,  8117],\n",
      "        [    0,     0,     0,  ...,    10,   335,     2],\n",
      "        [    0,     0,     0,  ...,    17,    19,  4319]])\n",
      "\n",
      "Sample label size:  torch.Size([1024])\n",
      "Sample label: \n",
      " tensor([0., 0., 0.,  ..., 1., 0., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)\n",
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(SentimentRNN, self).__init__()\n",
    "        self.output_szie = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #define layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           n_layers, \n",
    "                           batch_first = True, \n",
    "                           dropout = drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "#         print (\"x.size(0)\", x.size(0))\n",
    "        \n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.rnn(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.fc(lstm_out)\n",
    "        sig_out = self.sigmoid(out)\n",
    "        \n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(74074, 400)\n",
      "  (rnn): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)+1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "criterion  = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10... Step: 25... Loss: 0.626456... Val Loss: 0.634351\n",
      "Epoch: 3/10... Step: 50... Loss: 0.554207... Val Loss: 0.569299\n",
      "Epoch: 4/10... Step: 75... Loss: 0.455695... Val Loss: 0.525918\n",
      "Epoch: 6/10... Step: 100... Loss: 0.418175... Val Loss: 0.502986\n",
      "Epoch: 7/10... Step: 125... Loss: 0.706977... Val Loss: 0.599667\n",
      "Epoch: 8/10... Step: 150... Loss: 0.339981... Val Loss: 0.500383\n",
      "Epoch: 10/10... Step: 175... Loss: 0.299648... Val Loss: 0.522988\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "counter = 0\n",
    "print_every = 25\n",
    "clip = 5 # gradient clipping\n",
    "\n",
    "net = net.to(device)\n",
    "net.train()\n",
    "\n",
    "for e in range(epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        if (inputs.size(0)!= batch_size):\n",
    "            print(\"inputs.size(0)\",inputs.size(0))\n",
    "            continue\n",
    "            \n",
    "\n",
    "        h = tuple([each.data for each in h])\n",
    "        #print([each.size(1) for each in h])\n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs,h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        counter = counter + 1\n",
    "\n",
    "        if counter % print_every == 0:\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.489\n",
      "Test accuracy: 0.795\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 248, 19, 11, 29, 109, 114, 15, 389, 3, 11, 182, 61, 274, 145, 12, 19, 69, 77, 114, 3, 1, 411, 15, 540]]\n"
     ]
    }
   ],
   "source": [
    "# negative test review\n",
    "test_review_neg = 'The worst movie I have seen; acting was terrible and I want my money back . This movie had bad acting and the dialogue was slow .'\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([ word2int[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_review(test_review_neg)\n",
    "print(test_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   1 248  19  11  29\n",
      "  109 114  15 389   3  11 182  61 274 145  12  19  69  77 114   3   1 411\n",
      "   15 540]]\n"
     ]
    }
   ],
   "source": [
    "seq_length=200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_review, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.944326\n",
      "Positive review detected!\n"
     ]
    }
   ],
   "source": [
    "# positive test review\n",
    "test_review_pos = 'This movie had the best acting and the dialogue was so good'\n",
    "# call function\n",
    "seq_length=200 # good to use the length that was trained on\n",
    "\n",
    "predict(net, test_review_pos, seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (AI)",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
